\section{Introduction}
We study the type-inference problem for a dependently
typed calculus with \emph{alpha-equality} of types.

\paragraph{Dependently typed calculus.} We use a unified
representation for different syntactic levels (except polymorphic
types) by following the pure type system representation of the
calculus of constructions ($\lambda C$).

This system is economic in that one set of rules covers all syntactic
levels. However it also includes complexity when dealing with the
dependency of Pi types in type checking. For example:

\begin{lstlisting}
let f = \a. \b:a. b in \x. \y. f y x.
\end{lstlisting}

Here in the let body, \lst x cannot be of type \lst y because \lst y
does not exist when \lst x is introduced, so this program will be
rejected.

\paragraph{Let generalization.} Introducing let rule in a dependently
typed system needs extra effort. For example:

\begin{lstlisting}
let a = int in \x:a. x+1
\end{lstlisting}

This program type checks only if we know \lst{a} is just another
name for \lst{int} when infering \lst{x+1}. But in the usual let rule, we will
only pass \lst{a:*} to the let body. It is not enough here. So we 
store the whole let binding \lst{a:*=int} in the context and use the
binding when needed.

As in traditional Damas-Milner type inference, we do
polymorphic generalization at let bindings. So the expression:

\begin{lstlisting}
let id = \x. x in \x:(id nat). id 3
\end{lstlisting}

can type check. This is the only way polymorphism 
appears in the current type system. That is, polymorphism is rank-one
and can only be generated by the type system at let bingdings.
Note, however, that the system also include Pi types, and
higher-ranked polymorphism can be simulated using Pi types. 

\paragraph{Higher-kinds and dependent types} 
Since our type system uses a unified representation for different
syntactic levels, $\star$ (the type of types) is also regarded as a
value. Then we can easily achieve higher-kinded types such as those 
in $System F_{\omega}$.  Consider the program:

\begin{lstlisting}
let tc = \f . f int in \x : tc Id . x
\end{lstlisting}

Our system is capable of infering this example, where \lst {tc} has
type \lstinline{forall a:*. (* -> a) -> a}.

\begin{comment}
Furthermore, again due to the unified representation we can have 
some simple dependently typed programs such as:

\begin{lstlisting}
let tc = \f . f 0 in \x : tc Id . x
\end{lstlisting}
\end{comment}

\paragraph{Explicit type-level computation.} $\lambda C$ supports beta
equality of types. Which means, the following two types are equal
under beta reduction:

\begin{lstlisting}
(\x:*. x) int
int
\end{lstlisting}

While the beta conversion rule brings a lot of convenience, an
unfortunate consequence is that it makes decidability of type
checking much harder. For type-inference purposes, we believe
this also creates difficulties. For example, if we consider
traditional unification, the unification algorithm would need
to be able to unify the two types above. However, higher-order
unification is undecidable.

In our type system, we use type-safe cast construct $\castdn$ (beta
reduction) and $\ercastup$ (beta expansion) to make type computation
explicit. Which means, type equality is based on $\alpha$-equality and
type \lst{(\\x:*. x) int} cannot be regarded as type \lst{int}.

$\castdn$ operator makes the resulting type be a beta reduction of the
original type of the term. So

\begin{lstlisting}
e1 : (\x:*. x) int
e2 = $\castdn$e1 : int
\end{lstlisting}

$\ercastup$ operator makes the resulting type be a beta expansion of
the original type of the term. But for a specific type, there are
unlimited types that can be reduced to the original type. So when
using $\ercastup$, we need to provide the type we want to castup:

\begin{lstlisting}
$\ercastup$[(\x:*. x) int] e2 : (\x:*. x) int
$\ercastup$[(\x:int. int) 2] e2 : (\x:int. int) 2
\end{lstlisting}

Cast constructs allow only one-step reduction or expansion. So we have

\begin{lstlisting}
e1 : (\x:*. \y:int. x) int 2
$\castdn$e1 : (\y:int. int) 2
$\castdn \castdn$e1 : int
\end{lstlisting}

\paragraph{Difficulties}

One particular difficulty that we encountered was the influence of the
dependency of Pi type on throwing away trailing context and
destructing of function types.

Due to dependency, the return type of a dependently typed function
could refer to the outer bound variables. For example expression
\lst{\\x:*.\\y:x.y} of type $\bpi x \star {\bpi y x x}$. So while
infering a lambda case, it changes the order of the existential
variable that represents the return type, and the bound variable in
the context, which also results in a different way dealing with
trailing context. And also, while unifying an existential variable
with the above type, we cannot simply destruct the existential
variable into Pi type form like $\genA = \bpi x {\genA_1} {\genA_2}$,
because $\genA_2$ could mention $x$. We discuss this issue 
in more detail later.

\paragraph{Limitations so far}
One limitation is that currently the system does not support
higher-rank polymorphism. We attempted to extend the system
with higher-ranked polymorphism, but this seemed quite complex.
It remains to be seen whether the complexity is fundamental, or
whether we missed something. It may be the case that we need to
introduce some restrictions (for example predicativity).


\paragraph{Contributions.} In summary, the contributions of this work are:
\begin{itemize}
\item We give a set of algorithmic bidirectional typing rules for dependently typed calculus, where a unified representation is used for different syntactic levels with higher kind.
\item By using ordered contexts, dealing with let bindings is quite simple. And we preserve the Damas-Milner style of let polymorphism.
\item Explicit type casts allows type unification to be based on alpha equality and the decidability of type-level computation is achieved.
\end{itemize}
